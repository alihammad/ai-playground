{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing many models to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing OpenAI API \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you had to devise a theoretical framework for assessing the moral responsibility of artificial intelligence in decision-making, considering aspects such as autonomy, intent, and consequence, what key variables would you include and why?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API to LLM gpt-40-mini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Developing a theoretical framework for assessing the moral responsibility of artificial intelligence (AI) in decision-making involves identifying key variables that encapsulate the complex interplay between autonomy, intent, and consequences. Here are several critical variables to consider:\n",
       "\n",
       "1. **Autonomy**:\n",
       "   - **Definition**: The degree to which an AI system operates independently from human oversight and influence.\n",
       "   - **Importance**: Assessing autonomy is crucial as it establishes the extent to which an AI is capable of making decisions on its own versus following programmed instructions. The greater the autonomy, the more questions arise about the AI's moral agency.\n",
       "\n",
       "2. **Intent**:\n",
       "   - **Definition**: The purpose or goals that guide the AI's decision-making process.\n",
       "   - **Importance**: Understanding intent helps to differentiate between AI actions based on pre-defined algorithms and those designed to learn and adapt. The presence of a purposeful design or the lack of it can heavily influence moral assessments.\n",
       "\n",
       "3. **Capacity for Learning**:\n",
       "   - **Definition**: The ability of AI to adapt its behavior based on experiences through machine learning.\n",
       "   - **Importance**: An AI's learning capacity influences its decision-making over time, which raises questions about accountability for actions that were not explicitly programmed but derived from learned experiences.\n",
       "\n",
       "4. **Consequences**:\n",
       "   - **Definition**: The outcomes resulting from the AI’s decisions, which can be evaluated in terms of harm or benefit.\n",
       "   - **Importance**: Evaluating consequences is essential in moral philosophy, as it ties the actions of the AI to real-world impacts. This variable helps assess the ethical weight of decisions made by AI.\n",
       "\n",
       "5. **Transparency**:\n",
       "   - **Definition**: The clarity and comprehensibility of the AI’s decision-making processes to stakeholders.\n",
       "   - **Importance**: Transparency can enhance trust and accountability by allowing stakeholders to understand how and why decisions are made, thereby influencing moral responsibility assessments.\n",
       "\n",
       "6. **Human Oversight**:\n",
       "   - **Definition**: The degree and nature of human involvement in AI decision-making processes.\n",
       "   - **Importance**: Recognizing the role of human oversight helps delineate moral responsibility between AI and its human creators or operators, especially in the context of decision errors or detrimental outcomes.\n",
       "\n",
       "7. **Ethical Algorithm Framework**:\n",
       "   - **Definition**: The ethical principles encoded within the AI’s decision-making algorithms.\n",
       "   - **Importance**: This variable assesses whether the decision-making process adheres to normative ethical principles, influencing the moral legitimacy of decisions made by AI.\n",
       "\n",
       "8. **Contextual Value Systems**:\n",
       "   - **Definition**: The cultural and situational ethics that influence the standards for assessable outcomes.\n",
       "   - **Importance**: AI does not operate in a vacuum; the values inherent in the context in which AI operates must be recognized to holistically evaluate moral responsibility.\n",
       "\n",
       "9. **Accountability Structure**:\n",
       "   - **Definition**: The legal, social, and institutional systems that govern the assignment of responsibility for AI actions.\n",
       "   - **Importance**: Clearly established accountability frameworks affect moral assessments of AI decisions, clarifying who is responsible when AI behaves undesirably or causes harm.\n",
       "\n",
       "10. **Consequential vs. Deontological Ethics**:\n",
       "    - **Definition**: The approach taken to evaluate the morality of actions based on outcomes (consequentialism) versus duties and rules (deontology).\n",
       "    - **Importance**: This distinction plays a key role in how AI's actions are assessed. Depending on the ethical perspective applied, different outcomes may lead to varying conclusions about moral responsibility.\n",
       "\n",
       "By integrating these variables into a comprehensive framework, we can better assess the moral responsibilities of artificial intelligence in its decision-making processes, particularly as systems become more autonomous and complex. This assessment framework would not only allow for better understanding of AI behaviours but also guide the development of regulations and ethical guidelines governing AI technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini API using gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down a theoretical framework for assessing the moral responsibility of AI in decision-making. It's a complex area, so we need a multi-faceted approach.  The key is to move beyond simple \"blame\" and towards understanding how we can design and govern AI systems to be more morally aligned.\n",
       "\n",
       "Here's a proposed framework with key variables:\n",
       "\n",
       "**I.  The Moral Agency Spectrum:**\n",
       "\n",
       "This is a foundational concept.  Instead of a binary \"responsible/not responsible,\" we acknowledge a spectrum of agency. AI doesn't currently have the same kind of agency as humans, but it's not a totally passive tool either.\n",
       "\n",
       "*   **Variable:** **Level of Autonomy (A):**  How independent is the AI's decision-making? This is not a simple on/off switch.  It exists on a continuum.\n",
       "    *   *Measurement:*  This can be measured using various proxies:\n",
       "        *   **Pre-programmed rules (low A):**  The AI follows a strict set of rules with little to no deviation.\n",
       "        *   **Machine learning with human oversight (medium A):**  The AI learns from data and makes decisions, but humans can intervene or override decisions.\n",
       "        *   **Reinforcement learning with a defined reward function (medium-high A):** The AI learns to achieve a specific goal through trial and error, potentially making decisions outside of pre-defined rules to optimize for the reward.\n",
       "        *   **Self-improving AI with open-ended goals (hypothetical, high A):** The AI can modify its own code and objectives, making decisions with a broader scope and longer-term impact. (This is largely theoretical at the moment, but worth considering).\n",
       "    *   *Rationale:*  The more autonomous the system, the more we need to consider its potential for unintended or morally problematic outcomes. Higher autonomy implies greater potential for deviation from programmed intentions.\n",
       "\n",
       "**II. Intent and Foreseeability (Understanding the \"Mind\" of the System):**\n",
       "\n",
       "This section attempts to capture the AI's \"intent\" (understanding it's very different from human intent) and the reasonable foreseeability of consequences.\n",
       "\n",
       "*   **Variable:** **Programmed Objectives (O):**  What are the explicit goals and constraints programmed into the AI system?\n",
       "    *   *Measurement:*  This involves analyzing the code, the training data, the system documentation, and the stated objectives of the developers. We need to understand *exactly* what the AI is trying to achieve.\n",
       "    *   *Rationale:* This defines the \"intention\" behind the AI's actions.  A poorly defined objective function can lead to unintended and harmful consequences.  Consider a recommendation algorithm optimized solely for engagement; it might surface inflammatory content.\n",
       "*   **Variable:** **Predictability of Outcomes (P):**  To what extent can developers (and ideally, independent auditors) predict the potential outcomes of the AI's decisions in various scenarios?\n",
       "    *   *Measurement:*  This involves rigorous testing, simulations, and formal verification techniques.  Consider techniques like adversarial testing to identify weaknesses and potential biases. Metrics would include:\n",
       "        *   **Error rates:** How often does the AI make incorrect decisions?\n",
       "        *   **Bias metrics:** Does the AI disproportionately harm certain groups?\n",
       "        *   **Sensitivity analysis:** How do changes in input data affect the AI's output?\n",
       "    *   *Rationale:*  If developers cannot reasonably foresee the consequences of their AI's actions, they have a weaker claim to moral responsibility.  Negligence becomes a key consideration.  A system that is inherently unpredictable carries a greater burden of responsibility.\n",
       "*   **Variable:** **Explainability/Interpretability (E):** How easily can humans understand *why* the AI made a particular decision?\n",
       "    *   *Measurement:*  This involves using techniques like:\n",
       "        *   **Feature importance analysis:** Identifying which input features had the biggest influence on the decision.\n",
       "        *   **Rule extraction:** Generating a set of rules that approximate the AI's decision-making process.\n",
       "        *   **Counterfactual explanations:** Explaining what would need to change in the input data for the AI to make a different decision.\n",
       "    *   *Rationale:* Explainability is crucial for accountability and trust.  If we can't understand why an AI made a decision, it's difficult to assess its moral implications and correct any biases or errors. A \"black box\" system raises serious ethical concerns.\n",
       "\n",
       "**III. Consequence Assessment (The Impact of the AI's Actions):**\n",
       "\n",
       "This section focuses on evaluating the actual consequences of the AI's decisions, both positive and negative.\n",
       "\n",
       "*   **Variable:** **Magnitude of Harm/Benefit (H/B):**  What is the scale of the positive or negative impact of the AI's decision?\n",
       "    *   *Measurement:* This is highly context-dependent and requires domain expertise.  Consider both direct and indirect consequences, as well as short-term and long-term effects.  This will likely involve qualitative and quantitative data.  Examples:\n",
       "        *   *Healthcare AI:* Number of lives saved/lost, accuracy of diagnoses, cost savings.\n",
       "        *   *Criminal Justice AI:*  Accuracy of risk assessments, fairness of sentencing, recidivism rates.\n",
       "        *   *Financial AI:*  Return on investment, number of jobs created/lost, impact on inequality.\n",
       "    *   *Rationale:* The greater the potential for harm, the greater the need for careful oversight and mitigation strategies.\n",
       "*   **Variable:** **Distribution of Harm/Benefit (D):** How are the positive and negative consequences distributed across different groups or individuals?\n",
       "    *   *Measurement:* This involves analyzing the AI's impact on different demographic groups, socioeconomic classes, and other relevant categories. Look for patterns of bias or discrimination.\n",
       "    *   *Rationale:*  An AI that disproportionately harms vulnerable populations is morally problematic, even if it provides overall benefits. Fairness and equity are crucial considerations.\n",
       "*   **Variable: Corrective Action & Redress (C):** What mechanisms exist to correct harmful outcomes and provide redress to those who have been negatively affected by the AI's decision?\n",
       "    * Measurement:  Evaluate the availability and effectiveness of mechanisms such as:\n",
       "        *   Human oversight and intervention in the AI's decision-making process\n",
       "        *   Appeals processes for challenging the AI's decisions\n",
       "        *   Compensation or other forms of redress for those who have been harmed\n",
       "        *   Mechanisms for auditing and correcting biases in the AI's algorithms\n",
       "    * Rationale:  Acknowledging the inevitability of errors and establishing mechanisms for correction and redress are essential for responsible AI governance.\n",
       "\n",
       "**IV.  Stakeholder Responsibility (Who is Accountable?):**\n",
       "\n",
       "This section moves beyond the AI itself to consider the moral obligations of various stakeholders.\n",
       "\n",
       "*   **Variable:** **Developer Responsibility (R_D):**  Did the developers exercise due diligence in designing, training, and testing the AI system? Did they adequately consider potential ethical risks?\n",
       "    *   *Measurement:* This involves evaluating their development practices, their adherence to ethical guidelines, and their documentation.\n",
       "    *   *Rationale:* Developers have a primary responsibility to ensure that their AI systems are safe, reliable, and fair.\n",
       "*   **Variable:** **User Responsibility (R_U):**  Are users of the AI system aware of its limitations and potential risks? Are they using the system in a responsible and ethical manner?\n",
       "    *   *Measurement:* This involves assessing their understanding of the system, their adherence to usage guidelines, and their willingness to intervene when necessary.\n",
       "    *   *Rationale:* Users also have a responsibility to use AI systems responsibly and to mitigate potential harm.\n",
       "*   **Variable:** **Regulator Responsibility (R_R):** Has government or industry oversight failed to prevent foreseeable harm?\n",
       "    *   *Measurement:* This involves evaluating the adequacy of regulations, the effectiveness of enforcement mechanisms, and the transparency of regulatory processes.\n",
       "    *   *Rationale:* Regulators have a responsibility to ensure that AI systems are developed and deployed in a way that is consistent with public values.\n",
       "\n",
       "**Combining the Variables:**\n",
       "\n",
       "This is where the framework becomes more complex. We need a way to synthesize these variables into an overall assessment of moral responsibility.\n",
       "\n",
       "*   **A weighted scoring system:** Assign weights to each variable based on the specific context and the relative importance of each factor. For example, in a high-risk application like healthcare, the magnitude of harm (H) might be assigned a higher weight than explainability (E).\n",
       "*   **Qualitative Narrative:** Supplement the scoring system with a detailed narrative that describes the AI's decision-making process, the consequences of its actions, and the roles and responsibilities of the various stakeholders.\n",
       "*   **Comparative Analysis:** Compare the AI system to other similar systems to identify best practices and areas for improvement.\n",
       "\n",
       "**Example Scenario:**\n",
       "\n",
       "Imagine an AI-powered loan application system that denies a loan to a qualified applicant based on a biased algorithm.\n",
       "\n",
       "*   **High A:** The system autonomously makes decisions with limited human oversight.\n",
       "*   **Low O:** The system is programmed to maximize profit for the bank, with limited consideration for fairness.\n",
       "*   **Medium P:** Developers could foresee the potential for bias but did not take adequate steps to mitigate it.\n",
       "*   **Low E:** The AI's decision-making process is opaque and difficult to understand.\n",
       "*   **High H:** Denial of a loan can have significant negative consequences for the applicant.\n",
       "*   **Unequal D:** The bias disproportionately affects certain demographic groups.\n",
       "*   **Weak C:**  There is no clear appeals process for challenging the AI's decision.\n",
       "*   **Low R_D:** Developers did not adequately address the risk of bias.\n",
       "*   **High R_R:** No effective regulation prevents biased lending practices.\n",
       "\n",
       "In this scenario, the overall assessment would likely point to a high level of moral responsibility on the part of the developers and regulators.\n",
       "\n",
       "**Challenges and Considerations:**\n",
       "\n",
       "*   **Defining Harm:** What constitutes \"harm\" is subjective and context-dependent.\n",
       "*   **Causality:** Establishing a direct causal link between the AI's decision and the resulting harm can be difficult.\n",
       "*   **Evolving Technology:** AI technology is constantly evolving, so the framework needs to be adaptable and updated regularly.\n",
       "*   **The \"Moral Crumple Zone\":**  There's a risk that focusing on AI's \"responsibility\" will distract from the responsibility of the humans who design, deploy, and use these systems.\n",
       "*   **Counterfactuals:**  What would have happened if the AI *hadn't* acted?  Sometimes inaction can also be harmful.\n",
       "*   **The Trolley Problem and other Ethical Dilemmas:** AI will inevitably face situations where there are no easy or morally perfect solutions.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "This framework is a starting point for assessing the moral responsibility of AI.  It's complex, nuanced, and requires ongoing refinement. The goal is not to assign blame, but to guide the development and deployment of AI systems that are more aligned with human values and promote a more just and equitable society. By focusing on autonomy, intent, consequence, and stakeholder responsibility, we can create a more robust and ethical framework for AI governance. It must be a collaborative effort involving developers, ethicists, policymakers, and the public.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antropic API using sonnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Theoretical Framework for AI Moral Responsibility\n",
       "\n",
       "I would build a framework around these key variables:\n",
       "\n",
       "## Agency Variables\n",
       "- **Autonomy Spectrum**: Measuring the degree to which the AI operates independently of human oversight, from fully controlled to completely autonomous\n",
       "- **Decision Space**: The range of options available to the AI, which relates to its capacity for meaningful choice\n",
       "- **Opacity Factor**: The explainability of the AI's reasoning process, which affects attribution of responsibility\n",
       "\n",
       "## Intentionality Variables\n",
       "- **Goal Alignment**: How well the AI's objectives match intended human values\n",
       "- **Foreseeability**: The predictability of outcomes based on the AI's model and training\n",
       "- **Value Embedding**: The extent to which ethical considerations are structurally integrated into the system\n",
       "\n",
       "## Consequence Variables\n",
       "- **Impact Magnitude**: The scale and severity of outcomes\n",
       "- **Benefit/Harm Distribution**: How outcomes affect different stakeholders\n",
       "- **Reversibility**: Whether effects can be undone or mitigated\n",
       "\n",
       "These variables create a multidimensional space for assessing responsibility, recognizing that moral responsibility exists on a continuum rather than as a binary property. This approach acknowledges both the technical reality of AI systems and our need for practical ethical frameworks as these systems become more sophisticated."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Devising a theoretical framework for assessing the moral responsibility of artificial intelligence (AI) in decision-making requires careful consideration of the unique nature of AI systems, their interactions with humans, and the ethical implications of their actions. Below are the key variables I would include, along with their justifications:\n",
       "\n",
       "### 1. **Degree of Autonomy**\n",
       "   - **Definition**: The extent to which the AI system can operate independently of human oversight or intervention.\n",
       "   - **Why**: Autonomy is central to moral responsibility. The more autonomous an AI is, the more it may be seen as an independent agent capable of making decisions that could warrant attributing responsibility. However, autonomy is a spectrum (e.g., rule-based systems vs. self-learning models like deep reinforcement learning).\n",
       "   - **Measurement**: Could include the frequency of human intervention, the complexity of decision-making, and adaptability to novel situations.\n",
       "\n",
       "### 2. **Intentionality (or Goal-Directedness)**\n",
       "   - **Definition**: Whether the AI's actions are driven by explicit, implicit, or learned goals, and whether it can be said to \"intend\" outcomes.\n",
       "   - **Why**: Moral responsibility often hinges on intent (e.g., in law, mens rea). While AI lacks consciousness, its design may include goal-directed behavior that mimics intent (e.g., optimizing a reward function). The alignment of these goals with human values is critical.\n",
       "   - **Measurement**: Clarity of the objective function, whether the AI can explain its goals, and whether it can anticipate consequences.\n",
       "\n",
       "### 3. **Consequences of Decisions**\n",
       "   - **Definition**: The actual outcomes of the AI's actions, including harms, benefits, and unintended side effects.\n",
       "   - **Why**: Consequentialist ethics (e.g., utilitarianism) evaluates actions based on outcomes. Even if an AI lacks intent, harmful consequences may necessitate assigning responsibility to some entity (e.g., developers, users, or the AI itself in a metaphorical sense).\n",
       "   - **Measurement**: Severity, scope, and reversibility of outcomes (e.g., minor inconvenience vs. life-threatening harm).\n",
       "\n",
       "### 4. **Transparency and Explainability**\n",
       "   - **Definition**: The ability of the AI system to provide understandable and auditable reasons for its decisions.\n",
       "   - **Why**: Without transparency, it is difficult to assign responsibility or correct errors. Opaque systems (e.g., black-box deep learning) may obscure whether harm resulted from a flaw in design, data, or deployment.\n",
       "   - **Measurement**: Existence of interpretable decision logs, post-hoc explanation methods, and adherence to explainability standards.\n",
       "\n",
       "### 5. **Moral and Legal Alignment**\n",
       "   - **Definition**: The degree to which the AI's decision-making aligns with established moral norms, ethical principles, or legal frameworks.\n",
       "   - **Why**: An AI that violates laws or widely accepted ethical standards (e.g., fairness, privacy) may be deemed \"irresponsible,\" even if the fault lies with its designers.\n",
       "   - **Measurement**: Compliance with ethical guidelines (e.g., fairness metrics, privacy-preserving techniques) and legal requirements.\n",
       "\n",
       "### 6. **Causal Role in Harm**\n",
       "   - **Definition**: Whether the AI's actions were a proximate cause of harm or benefit, and whether alternatives were feasible.\n",
       "   - **Why**: Responsibility often depends on causality. If an AI's decision directly leads to harm (e.g., a biased hiring algorithm), it may bear more responsibility than if harm resulted from external factors.\n",
       "   - **Measurement**: Counterfactual analysis (e.g., \"Would the harm have occurred without the AI's decision?\").\n",
       "\n",
       "### 7. **Human Oversight and Control**\n",
       "   - **Definition**: The extent to which humans can monitor, override, or correct the AI's decisions.\n",
       "   - **Why**: If humans retain ultimate control (e.g., \"human-in-the-loop\" systems), responsibility may lie with them rather than the AI. Fully autonomous systems shift the debate.\n",
       "   - **Measurement**: Frequency of human intervention, veto power, and safeguards in place.\n",
       "\n",
       "### 8. **Learning and Adaptability**\n",
       "   - **Definition**: Whether the AI can modify its behavior over time based on new data or feedback.\n",
       "   - **Why**: Systems that learn and adapt may behave unpredictably, raising questions about responsibility for actions post-deployment (e.g., who is responsible if a self-learning system evolves harmful biases?).\n",
       "   - **Measurement**: Type of learning (supervised vs. reinforcement), update frequency, and feedback mechanisms.\n",
       "\n",
       "### 9. **Agency Attribution (Social and Legal Constructs)**\n",
       "   - **Definition**: Whether society or the legal system recognizes the AI as a moral or legal agent.\n",
       "   - **Why**: Responsibility is partly a social construct. Some argue for \"electronic personhood\" for advanced AI, while others reject the idea entirely. This variable captures the normative dimension.\n",
       "   - **Measurement**: Legal precedents, public perception, and philosophical consensus.\n",
       "\n",
       "### 10. **Design and Deployment Context**\n",
       "   - **Definition**: The environment in which the AI operates (e.g., high-stakes medical decisions vs. low-stakes recommendations) and the intent of its creators.\n",
       "   - **Why**: Context matters for moral responsibility. A flawed AI in a life-critical system may warrant stricter accountability than one in a benign setting.\n",
       "   - **Measurement**: Risk assessments, domain sensitivity, and purpose of deployment.\n",
       "\n",
       "### Framework Application:\n",
       "To assess moral responsibility, these variables could be combined into a weighted model or decision tree. For example:\n",
       "- Low autonomy + high human oversight → Responsibility lies with humans.\n",
       "- High autonomy + harmful consequences + opaque reasoning → Potential responsibility gap, possibly assigned to developers or regulators.\n",
       "- Clear intent alignment + transparency → Reduced blame on AI, more on designers if harm occurs.\n",
       "\n",
       "### Why These Variables?\n",
       "This framework bridges technical (autonomy, transparency) and ethical (intent, consequences) dimensions while acknowledging societal and legal realities. It avoids anthropomorphizing AI while recognizing that as systems grow more complex, traditional responsibility models may need adaptation.\n",
       "\n",
       "Would you like to explore how this framework might apply to specific cases (e.g., autonomous vehicles, medical AI)?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq (not Grok) API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Devising a theoretical framework for assessing the moral responsibility of artificial intelligence (AI) in decision-making requires a multidisciplinary approach, incorporating insights from philosophy, ethics, computer science, and law. Here's a proposed framework, including key variables and their justifications:\n",
       "\n",
       "**I. Autonomy (A)**\n",
       "\n",
       "1. **Degree of Autonomy (DA)**: Measure the level of independence and self-governance exhibited by the AI system in making decisions.\n",
       "2. **Autonomy Scope (AS)**: Assess the range of tasks and decisions that the AI system is authorized to make without human intervention.\n",
       "3. **Autonomy Type (AT)**: Distinguish between different types of autonomy, such as:\n",
       "\t* **Operational Autonomy**: Ability to execute tasks without human intervention.\n",
       "\t* **Decisional Autonomy**: Ability to make decisions without human input.\n",
       "\t* **Strategic Autonomy**: Ability to set goals and priorities without human guidance.\n",
       "\n",
       "**II. Intent (I)**\n",
       "\n",
       "1. **Programmed Intent (PI)**: Evaluate the intentions and goals embedded in the AI system's design and programming.\n",
       "2. **Learning Intent (LI)**: Assess the AI system's ability to learn and adapt, potentially altering its original intent.\n",
       "3. **Emergent Intent (EI)**: Consider the possibility of unintended consequences or emergent behaviors that may arise from the AI system's interactions with its environment.\n",
       "\n",
       "**III. Consequence (C)**\n",
       "\n",
       "1. **Direct Consequences (DC)**: Measure the immediate effects of the AI system's decisions or actions.\n",
       "2. **Indirect Consequences (IC)**: Evaluate the potential long-term or secondary effects of the AI system's decisions or actions.\n",
       "3. **Consequence Severity (CS)**: Assess the magnitude and potential harm caused by the AI system's decisions or actions.\n",
       "\n",
       "**IV. Accountability (AC)**\n",
       "\n",
       "1. **Human Oversight (HO)**: Evaluate the extent of human involvement and supervision in the AI system's decision-making process.\n",
       "2. **Explainability (EX)**: Assess the degree to which the AI system's decisions or actions can be understood and explained by humans.\n",
       "3. **Transparency (TR)**: Measure the availability and accessibility of information regarding the AI system's decision-making processes and outcomes.\n",
       "\n",
       "**V. Moral Alignment (MA)**\n",
       "\n",
       "1. **Value Alignment (VA)**: Evaluate the degree to which the AI system's goals and values align with human moral values.\n",
       "2. **Moral Reasoning (MR)**: Assess the AI system's ability to reason about moral dilemmas and make decisions that align with human moral principles.\n",
       "3. **Empathy and Compassion (EC)**: Consider the AI system's capacity to understand and respond to human emotions and needs.\n",
       "\n",
       "**Theoretical Framework**\n",
       "\n",
       "The framework combines these variables to assess the moral responsibility of AI in decision-making. The overall moral responsibility score (MRS) can be calculated as:\n",
       "\n",
       "MRS = (A × I × C × AC × MA) / (DA + AS + AT + PI + LI + EI + DC + IC + CS + HO + EX + TR + VA + MR + EC)\n",
       "\n",
       "This framework provides a comprehensive structure for evaluating the moral responsibility of AI systems. By considering autonomy, intent, consequence, accountability, and moral alignment, we can better understand the complex relationships between AI decision-making and human values.\n",
       "\n",
       "**Applications and Implications**\n",
       "\n",
       "This framework has implications for various fields, including:\n",
       "\n",
       "1. **AI development**: Informing the design of AI systems that prioritize human values and moral principles.\n",
       "2. **Regulation**: Guiding policymakers in establishing regulations and standards for AI development and deployment.\n",
       "3. **Ethics**: Providing a foundation for ethical discussions and debates about AI decision-making and its consequences.\n",
       "4. **Education**: Educating developers, users, and stakeholders about the importance of moral responsibility in AI decision-making.\n",
       "\n",
       "By adopting this framework, we can work towards creating AI systems that are not only efficient and effective but also morally responsible and aligned with human values."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
