{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "Openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "cv = PdfReader(\"me/ali_baig_cv.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "+61434698442 (Mobile)\n",
      "ali.hammadbaig@gmail.com\n",
      "www.linkedin.com/in/\n",
      "alihammadbaig (LinkedIn)\n",
      "Top Skills\n",
      "Mentoring\n",
      "Cloud Applications\n",
      "Data Analytics\n",
      "Languages\n",
      "English (Full Professional)\n",
      "Punjabi (Full Professional)\n",
      "Urdu (Full Professional)\n",
      "Certifications\n",
      "Teradata 14 Certified Professional\n",
      "Hadoop 2.0 Java Developer\n",
      "Certification\n",
      "Hortonworks Hadoop 2.0 Developer\n",
      "Ali Baig\n",
      "Tech Leadership | Modern Data Platforms | Analytics | Warehousing\n",
      "| Big Data | Transformation\n",
      "Sydney, New South Wales, Australia\n",
      "Summary\n",
      "Solution Architect | Data Architect | Data Analytics | Data\n",
      "Engineering | Big Data | Hadoop ecosystem| AWS and Google Cloud\n",
      "| Automation\n",
      "• 14 years of IT experience in software application development, data\n",
      "warehousing, data analysis and data engineering.\n",
      "• Experienced in driving insights from petabyte-scale data lakes and\n",
      "data warehouses\n",
      "• Highly experienced in building data pipelines using a variety of\n",
      "tools and technologies\n",
      "• Experience in designing the modern data architecture \n",
      "• Experience in building cloud infrastructure for data ingestion and\n",
      "data analysis\n",
      "Core competencies:\n",
      "Hadoop Ecosystem - Cloudera/Hortworks Hadoop, Hive, HDFS,\n",
      "Spark, PySpark, Impala\n",
      "Cloud - AWS, GCP \n",
      "Programming - Python, Java, SQL, Unix Scripting\n",
      "Automation: Terraform, Ansible\n",
      "Others: Airflow/GCP Cloud Composer(Managed Airflow),\n",
      "Containerisation and Docker, Google BigQuery, AWS EMR\n",
      "Development Tools - Subversion (SVN), GIT, Bitbucket\n",
      "Experience\n",
      "Equifax - Australia\n",
      "5 years 5 months\n",
      "Application Engineering Manager\n",
      "May 2022 - Present (3 years 1 month)\n",
      "  Page 1 of 6   \n",
      "As application engineering manager, I am managing multiple cross-functional\n",
      "local and remote transformation teams that are building the next generation of\n",
      "credit ratings.  This transformation program will define how Equifax will build its\n",
      "credit score in future. \n",
      "• Manage three teams of software engineers, data engineers, SQL developers\n",
      "and SREs that are based in different time zones. \n",
      "• Manage internal and external stakeholders, vendors and consultants. \n",
      "• Work with technical and business BAs to gather and understand\n",
      "requirements and work with tech leads and scrum masters to plan activities\n",
      "and set priorities. \n",
      "• Regularly sync up with internal downstream business teams to showcase the\n",
      "solution and gather feedback.\n",
      "• Be a technical leader, contribute to solution design and architecture\n",
      "Squad Leader - Data Engineering\n",
      "January 2020 - May 2022 (2 years 5 months)\n",
      "Sydney, Australia\n",
      "At Equifax, I am leading a data squad comprising of highly proficient data\n",
      "engineers, SREs, QAs, Hadoop administrators and BAs. The squad is\n",
      "responsible for the end-to-end management of a product stream. My\n",
      "responsibilities include. \n",
      "• Manage the delivery of the engineers in my team, working broadly to manage\n",
      "stakeholders internal and external, understand and drive requirements, set\n",
      "priorities and communicate following an agile process.\n",
      "• Be a technical leader, participating in solution design and architecture\n",
      "reviews and maturing the systems and capabilities that I own.\n",
      "• Work with BAs and Scrum masters to refine user stories, organize, and\n",
      "oversee development sprints, work on monthly, quarterly, and yearly planning\n",
      "and ensure the squad delivers consistently and on time.\n",
      "• Owning the operations of four data platforms built using GCP, AWS, on-prem\n",
      "Cloudera Hadoop cluster, datalake and multiple products.\n",
      "• I own a data migration function as we are transitioning from on-prem\n",
      "solutions to cloud-native ones. \n",
      "• Work directly with businesses to understand their needs and build solutions\n",
      "with the help of local and global product and engineering teams.\n",
      "• Being a technology enthusiast with an extensive coding background, I love\n",
      "rolling up my sleeves and cutting code when needed. Also help the team\n",
      "debug complex problems, review solutions and solve any technical issues. \n",
      "  Page 2 of 6   \n",
      "• Remove Blockers and escalate concerns/team issues/blockers\n",
      "• Responsible for hiring and coaching team members\n",
      "NSW Department of Customer Service\n",
      "Senior Data Analyst/Data Engineer\n",
      "June 2017 - December 2019 (2 years 7 months)\n",
      "Sydney, Australia\n",
      "-------------------------\n",
      "Government agencies own a large amount of public sector data which is\n",
      "complex and hard to\n",
      "understand.\n",
      "In my role as a Senior Data Analyst, I liaised with the state government\n",
      "agencies to understand\n",
      "complicated business requirements and problems, analyze complex, large and\n",
      "versatile data to find\n",
      "meaningful trends, and consequently solutionize new ideas and approaches.\n",
      "As part of the team, I had built end-to-end modern big data pipelines to extract\n",
      "and load data into\n",
      "Hadoop and Azure data lakes from a variety of sources, stitched complex\n",
      "datasets together, transformed\n",
      "them using variety of tools and technologies, cleansed and prepare data for\n",
      "analysis, performed\n",
      "exploratory data analysis and apply machine learning models. \n",
      "I use   ,   , ,     to build the solution. To visualize the data, I used Matplotlib,\n",
      "Tableau and MS Power BI.\n",
      "I also acted as a mentor to scholars on various projects during my role as a\n",
      "data analyst. I believe this enhanced my capability to guide those junior and\n",
      "help them to become better professionals.\n",
      "DAC followed agile practices. Project planning was divided into two-week\n",
      "sprints, followed by fortnightly showcases to the customers, retros and next\n",
      "sprint planning meeting.\n",
      "  Page 3 of 6   \n",
      "--------------------\n",
      "As a DevOps engineer, I worked in the Service Management team to manage\n",
      "Hadoop platform\n",
      "operations. I was actively involved in cross-functional BAU activities directly\n",
      "impacting DAC’s\n",
      "operations, capabilities and customers. A huge success for me was to\n",
      "operationalize Hadoop for\n",
      "analytical use cases, and I was actively involved in the Operation’s solution\n",
      "design and development.\n",
      "----------------\n",
      "My role was to design and implement the data governance framework in a big\n",
      "data environment on Hadoop and Azure cloud.\n",
      "Optus\n",
      "Insights Developer\n",
      "April 2016 - March 2017 (1 year)\n",
      "Sydney, Australia\n",
      "• As a data analyst, provided insights to stakeholders using SQL scripts and\n",
      "procedure \n",
      "• Built and re-mediated reports \n",
      "• Ensured right time data availability and high data quality\n",
      "• Automating development environment switching from Dev to SIT and UAT\n",
      "using Python and UNIX scripting\n",
      "Teradata\n",
      "Professional Services Consultant\n",
      "November 2013 - March 2017 (3 years 5 months)\n",
      "Islamabad, Pakistan\n",
      "•     Teradata 13, 15, 15 \n",
      "•     Teradata tools an utilities including BTEQ and TPT\n",
      "•     ETL using Datastage, Informatica and Teradata Global Control Framework\n",
      "•     Data warehouse/mart development using Kimball, Inmon and data vault\n",
      "modelling techniques\n",
      "Commonwealth Bank\n",
      "Teradata Consultant\n",
      "October 2015 - March 2016 (6 months)\n",
      "  Page 4 of 6   \n",
      "Sydney, Australia\n",
      "• Developed data warehouse usage report using Teradata SQL, Excel and\n",
      "VBA\n",
      "• Designed and developed new Teradata row level security scheme for data\n",
      "protection in GDW1.0 and 2.0 using Teradata SQL, Stored Procedures and\n",
      "Shell scripting\n",
      "• Clarifying business rules, gathering and documenting detailed business\n",
      "requirement from stakeholders\n",
      "ANZ\n",
      "Teradata Consultant\n",
      "August 2015 - September 2015 (2 months)\n",
      "• Remediation of the client’s Teradata data warehouse\n",
      "• Profiling data and identifying problems in the data\n",
      "• Design & development of solutions for identified problems\n",
      "• Loading and transforming data using Teradata Global Control Framework\n",
      "(GCFR)\n",
      "• Developed load and transform jobs in IBM Datastage\n",
      "• Working at the client site and in constant communication with the client.\n",
      "eMumba\n",
      "Sr. Software Engineer\n",
      "February 2012 - October 2013 (1 year 9 months)\n",
      "Islamabad\n",
      "I joined eMumba when it a startup and now it has grown into a established\n",
      "company.I did some exciting work here which includes\n",
      "• Worked on advance open source technological stack\n",
      "• Web application development\n",
      "• Worked in small but geographically distributed team\n",
      "• Tools & Technologies: Java, JavaEE, MySQL, Spring framework, Hibernate,\n",
      "JavaScript, jQuery, HTML and CSS\n",
      "Teradata Global Consulting Center Islamabad Pakistan\n",
      "Professional Service Consultant and Software Developer\n",
      "February 2010 - January 2012 (2 years)\n",
      "Islamabad\n",
      "Working as a Professional Service Consultant and Software Developer\n",
      "in Teradata Corporation Global Consulting Center, Islamabad, Pakistan\n",
      "www.teradata.com\n",
      "  Page 5 of 6   \n",
      "The job responsibilities include\n",
      "- Coordinating with Teradata research centre in USA to develop a  Teradata\n",
      "Migration Suit.\n",
      "- Developing and API which translate Oracle scripts (Plain SQL, PL/SQL and\n",
      "SQL Plus) into Teradata equivalent scripts. \n",
      "- Programming languages used in this project include Java SE, TAWK\n",
      "programming language, Oracle and Teradata DBMS.\n",
      "Digital Processing Systems\n",
      "Software Engineer\n",
      "June 2007 - December 2009 (2 years 7 months)\n",
      "Islamabad\n",
      "Job Resonsibilities\n",
      "- Java and JavaEE \n",
      "- Extensive use of SQL and PLSQL\n",
      "- Java Script\n",
      "Education\n",
      "National University of Science and Technology\n",
      "Bachelor of Information Technology, Computer Science and\n",
      "Management · (2001 - 2006)\n",
      "Fauji Foundation Boys College Rawalpindi\n",
      "Faculty of Engineering, Physics, Chemistry, Mathematics · (1999 - 2000)\n",
      "  Page 6 of 6\n"
     ]
    }
   ],
   "source": [
    "print(linkedin\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
